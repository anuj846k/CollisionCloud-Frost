# Accident Reconstruction Tool - Complete Guide

## üéØ Why This Project Exists

### The Problem

After a car crash, people need answers:
- **When did it happen?** (exact time)
- **How fast were the cars going?**
- **Who was at fault?**
- **What exactly happened?**

**Currently, investigators:**
- Watch hours of video manually
- Write reports by hand
- Calculate speeds manually
- Take **days/weeks** to finish

### The Solution

This tool:
- Takes a **5-second crash video**
- Automatically finds **when the crash happened**
- Calculates **car speeds**
- Generates a **complete report** in **minutes**

---

## üìã Step-by-Step Process

### Step 1: Upload Video

**What happens:**
- User uploads a 5-second dashcam/CCTV video
- System saves it to cloud storage

**Why:**
- Need the video to analyze

**Example:**
```
User: "Here's my crash video" ‚Üí Uploads crash.mp4
System: "Got it! Saved to cloud"
```

---

### Step 2: Extract First Frame

**What happens:**
- System takes a screenshot from the first frame
- Saves it as an image

**Why:**
- Need a still image for calibration

**Example:**
```
System: "Taking screenshot of frame 1..."
Result: screenshot.jpg (shows the intersection/road)
```

---

### Step 3: Set Location

**What happens:**
- User finds the location on Google Maps
- System saves the GPS coordinates

**Why:**
- Need to know where the video was taken

**Example:**
```
User: "This happened at Main St & Oak Ave"
System: "Location saved: 37.7749¬∞N, 122.4194¬∞W"
```

---

### Step 4: Calibration (Homography) üîë

**What happens:**
- User marks 4+ matching points: one on the video frame, the same point on the map
- System learns how video pixels map to real-world GPS

**Why:**
- Without this, you only have pixels, not real-world locations
- With this, you can convert pixels to GPS and calculate real speeds

**Example:**
```
User clicks on video: "This building corner" ‚Üí Pixel (500, 300)
User clicks on map: "Same building corner" ‚Üí GPS (37.7749¬∞N, 122.4194¬∞W)

System learns: "Pixel (500, 300) = GPS (37.7749¬∞N, 122.4194¬∞W)"

User does this 3 more times with different points.
System now knows how to convert ANY pixel ‚Üí GPS location
```

**Visual Example:**
```
VIDEO FRAME (what camera sees):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üè¢ Building               ‚îÇ
‚îÇ     ‚¨áÔ∏è Point A              ‚îÇ
‚îÇ                            ‚îÇ
‚îÇ  üöó Car at (600, 400)       ‚îÇ  ‚Üê We want to know where this is in real world
‚îÇ                            ‚îÇ
‚îÇ  üè™ Store                  ‚îÇ
‚îÇ     ‚¨áÔ∏è Point B              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

GOOGLE MAP (real world):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üè¢ Building                ‚îÇ
‚îÇ     ‚¨áÔ∏è Point A (GPS)         ‚îÇ
‚îÇ                            ‚îÇ
‚îÇ  [Car's real location?]     ‚îÇ  ‚Üê System calculates this using matrix
‚îÇ                            ‚îÇ
‚îÇ  üè™ Store                   ‚îÇ
‚îÇ     ‚¨áÔ∏è Point B (GPS)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**After calibration:**
- Car at pixel (600, 400) ‚Üí System uses matrix ‚Üí Car at GPS (37.7750¬∞N, 122.4195¬∞W)

---

### Step 5: Process Video

**What happens:**
- System runs **YOLO** to detect vehicles in each frame
- System runs **ByteTrack** to track each vehicle across frames (assigns IDs)
- System saves all detections to the database

**Why:**
- Need to know where each car is in every frame

**Example:**
```
Frame 1: Car detected at (600, 400) ‚Üí Track ID #7
Frame 2: Car detected at (610, 405) ‚Üí Still Track ID #7 (same car)
Frame 3: Car detected at (620, 410) ‚Üí Still Track ID #7

Another car:
Frame 1: Car detected at (200, 300) ‚Üí Track ID #14
Frame 2: Car detected at (190, 295) ‚Üí Still Track ID #14
```

**Result:**
- Database has every car's position in every frame

---

### Step 6: Calculate Speeds

**What happens:**
- For each tracked vehicle:
  1. Get position in frame 10 (pixel coordinates)
  2. Convert pixel ‚Üí GPS using the calibration matrix
  3. Get position in frame 20 (pixel coordinates)
  4. Convert pixel ‚Üí GPS
  5. Calculate distance between GPS points
  6. Calculate speed: **distance √∑ time**

**Why:**
- Need real speeds for the report

**Example:**
```
Car #7:
- Frame 10: Pixel (600, 400) ‚Üí GPS (37.7749¬∞N, 122.4194¬∞W)
- Frame 20: Pixel (650, 450) ‚Üí GPS (37.7750¬∞N, 122.4195¬∞W)
- Distance: 0.1 miles
- Time: 0.33 seconds (10 frames √∑ 30 fps)
- Speed: 0.1 miles √∑ 0.33 seconds = 27 MPH
```

**Result:**
- Every car's speed at every moment

---

### Step 7: Detect Collision

**What happens:**
- System compares vehicles frame by frame:
  - Checks if bounding boxes overlap (IoU)
  - Checks distance between car centers
- If overlap is high and distance is low ‚Üí **collision detected**
- System finds:
  - **First contact frame**
  - **Peak impact frame**
  - **Separation frame**

**Why:**
- Need to know exactly when the crash happened

**Example:**
```
Frame 49:
- Car #7 box: (600, 400, 100, 80)
- Car #14 box: (650, 420, 100, 80)
- Overlap: 30% (high!)
- Distance: 50 pixels (close!)
- Result: COLLISION DETECTED!

First contact: Frame 49
Peak impact: Frame 52
Separation: Frame 60
```

---

### Step 8: AI Analysis

**What happens:**
- System sends all data to **AWS Bedrock Claude** (AI)
- Claude analyzes:
  - Detection data
  - Speeds
  - Collision details
  - Weather (if available)
- Claude generates a **natural language report**

**Why:**
- Need a readable explanation, not just numbers

**Example:**
```
Claude receives:
- Car #7: 27 MPH
- Car #14: 25 MPH
- Collision at frame 49
- Location: Main St & Oak Ave

Claude generates:
"At 1.6 seconds into the video, Vehicle #7 traveling at 27 MPH 
collided with Vehicle #14 traveling at 25 MPH. The collision 
occurred at the intersection of Main St and Oak Ave. Vehicle #7 
appears to have been making a left turn when Vehicle #14, 
traveling straight, could not stop in time..."
```

---

### Step 9: Generate PDF Report

**What happens:**
- System creates a PDF with:
  - Screenshot of collision frame
  - Map showing vehicle paths
  - AI-generated narrative
  - Technical details (speeds, times, coordinates)

**Why:**
- Need a final document for insurance/legal use

**Example:**
```
PDF contains:
1. Cover page: "Accident Report - Main St & Oak Ave"
2. Collision screenshot (frame 49)
3. Map with car trajectories drawn
4. AI narrative (full story)
5. Technical appendix (speeds, times, GPS coordinates)
```

---

## üîÑ Complete Flow (Simple Version)

```
1. UPLOAD VIDEO
   ‚Üì
2. EXTRACT FRAME (screenshot)
   ‚Üì
3. SET LOCATION (Google Maps)
   ‚Üì
4. CALIBRATE (mark 4+ points: video ‚Üí map)
   ‚Üì
5. PROCESS VIDEO (detect & track cars)
   ‚Üì
6. CALCULATE SPEEDS (pixels ‚Üí GPS ‚Üí MPH)
   ‚Üì
7. DETECT COLLISION (find when cars hit)
   ‚Üì
8. AI ANALYSIS (Claude writes report)
   ‚Üì
9. GENERATE PDF (final document)
```

---

## üìä Why Each Step Matters

| Step | Why It's Needed |
|------|----------------|
| **Upload video** | Need the source material |
| **Extract frame** | Need image for calibration |
| **Set location** | Need to know where it happened |
| **Calibrate** | **Without this, can't calculate real speeds** |
| **Process video** | Need to find all cars in all frames |
| **Calculate speeds** | Need real speeds for report |
| **Detect collision** | Need exact moment of crash |
| **AI analysis** | Need readable explanation |
| **Generate PDF** | Need final document |

---

## üåç Real-World Example

**Scenario:** Two cars crash at an intersection

1. **Upload:** User uploads 5-second dashcam video
2. **Extract frame:** System takes screenshot of intersection
3. **Set location:** User finds "Main St & Oak Ave" on map
4. **Calibrate:** User marks 4 building corners (video ‚Üí map)
5. **Process:** System finds Car #7 and Car #14 in every frame
6. **Calculate speeds:** Car #7 = 27 MPH, Car #14 = 25 MPH
7. **Detect collision:** Crash at frame 49 (1.6 seconds in)
8. **AI analysis:** Claude writes: "Car #7 turning left, Car #14 couldn't stop..."
9. **Generate PDF:** Final report ready for insurance

**Result:** Complete accident report in **5 minutes** instead of **5 hours**

---

## üîë Key Concepts Explained

### YOLO (You Only Look Once)
- AI model that detects objects in images/video
- Outputs: "There's a car at coordinates (x, y, width, height)"

### ByteTrack
- Tracking algorithm that follows the same vehicle across frames
- Assigns consistent IDs (e.g., Vehicle #7 stays #7 throughout)

### Bounding Box
- Rectangle around a detected object
- Format: (x, y, width, height) in pixels

### IoU (Intersection over Union)
- Measures how much two boxes overlap
- Range: 0 (no overlap) to 1 (perfect overlap)
- Used to detect collisions

### Homography
- Mathematical transformation that maps points from one view to another
- In this project: **video pixels ‚Üí real-world GPS coordinates**
- Requires at least 4 point pairs to calculate

### Homography Matrix
- 3√ó3 matrix that performs the transformation
- Calculated from the point pairs you mark

### Kalman Filter
- Smoothing algorithm that reduces jitter in bounding boxes
- Makes speed calculations more stable

---

## üí° Summary

**Problem:** Manual accident analysis is slow and expensive

**Solution:** Automate it with AI and computer vision

**Process:** Upload ‚Üí Calibrate ‚Üí Process ‚Üí Analyze ‚Üí Report

**Outcome:** Fast, accurate accident reports

---

## üõ†Ô∏è Original Project Tech Stack

### Frontend
- **React Router v7** - Web framework for the user interface
- **TypeScript** - Programming language (typed JavaScript)
- **Mantine UI** - Component library for buttons, forms, etc.
- **Vite** - Build tool for fast development
- **TanStack Query** - Data fetching and caching

### Backend
- **FastAPI** - Python web framework (creates REST APIs)
- **SQLAlchemy** - Database ORM (talks to PostgreSQL)
- **Pydantic** - Data validation
- **Python 3.11** - Programming language

### Video Processing
- **YOLOv8** - Object detection (finds cars in video)
- **ByteTrack** - Multi-object tracking (follows cars across frames)
- **Supervision** - Computer vision utilities
- **OpenCV** - Image/video processing
- **Kalman Filter** - Smoothing algorithm (reduces jitter)

### AI & Analysis
- **AWS Bedrock Claude Sonnet** - Large Language Model (writes reports)
- **Claude Agent Framework** - AI agent with tools (load_detections, compute_pair_metrics, etc.)

### Storage & Infrastructure
- **PostgreSQL** - Database (stores projects, detections, analysis)
- **AWS S3** - Cloud storage (stores videos, images, JSONL files)
- **Redis** - Message queue (for Celery tasks and SSE events)

### Background Processing
- **Celery** - Task queue system (runs video processing in background)
- **Redis** - Message broker (connects Celery workers)

### Report Generation
- **WeasyPrint** - PDF generation library
- **boto3** - AWS SDK (talks to S3 and Bedrock)

### Other Tools
- **Google Maps API** - For location search and map display
- **SSE (Server-Sent Events)** - Real-time streaming (shows Claude's thinking)
- **uv** - Python package manager

---

## üì¶ What Each Technology Does

| Technology | Purpose |
|------------|---------|
| **React Router** | Creates the web interface users interact with |
| **FastAPI** | Handles API requests (upload video, start processing, etc.) |
| **YOLOv8** | Detects vehicles in video frames |
| **ByteTrack** | Tracks the same vehicle across multiple frames |
| **PostgreSQL** | Stores all data (projects, detections, analysis results) |
| **AWS S3** | Stores large files (videos, images, JSONL detection files) |
| **AWS Bedrock Claude** | AI that analyzes data and writes reports |
| **Celery** | Runs long tasks (video processing) without blocking the API |
| **Redis** | Queue system for Celery tasks |
| **WeasyPrint** | Creates PDF reports |
| **Google Maps API** | Shows map for location selection and homography calibration |

---

## üèóÔ∏è Original Project Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FRONTEND (React)                      ‚îÇ
‚îÇ  - User uploads video                                    ‚îÇ
‚îÇ  - Sets location on Google Maps                          ‚îÇ
‚îÇ  - Marks calibration points                              ‚îÇ
‚îÇ  - Views results and PDF                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ HTTP Requests
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  BACKEND (FastAPI)                        ‚îÇ
‚îÇ  - Handles API requests                                  ‚îÇ
‚îÇ  - Manages projects and users                            ‚îÇ
‚îÇ  - Stores data in PostgreSQL                             ‚îÇ
‚îÇ  - Sends tasks to Celery                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                     ‚îÇ
        ‚ñº                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PostgreSQL  ‚îÇ    ‚îÇ  Redis Queue     ‚îÇ
‚îÇ  (Database)  ‚îÇ    ‚îÇ  (Task Queue)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Celery Worker   ‚îÇ
                    ‚îÇ  - Processes video‚îÇ
                    ‚îÇ  - Runs YOLO      ‚îÇ
                    ‚îÇ  - Calls Claude   ‚îÇ
                    ‚îÇ  - Generates PDF  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                    ‚îÇ                    ‚îÇ
        ‚ñº                    ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   AWS S3     ‚îÇ    ‚îÇ AWS Bedrock  ‚îÇ    ‚îÇ  PostgreSQL  ‚îÇ
‚îÇ  (Storage)   ‚îÇ    ‚îÇ   Claude     ‚îÇ    ‚îÇ  (Results)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Data Flow in Original Project

1. **User uploads video** ‚Üí FastAPI ‚Üí Saves to AWS S3
2. **User sets location** ‚Üí FastAPI ‚Üí Saves to PostgreSQL
3. **User calibrates** ‚Üí FastAPI ‚Üí Calculates homography matrix ‚Üí Saves to PostgreSQL
4. **User starts processing** ‚Üí FastAPI ‚Üí Sends task to Redis ‚Üí Celery worker picks up
5. **Celery processes video** ‚Üí Downloads from S3 ‚Üí Runs YOLO + ByteTrack ‚Üí Saves detections to PostgreSQL + JSONL to S3
6. **User starts AI analysis** ‚Üí FastAPI ‚Üí Sends task to Redis ‚Üí Celery worker ‚Üí Calls AWS Bedrock Claude ‚Üí Streams results via SSE
7. **User generates PDF** ‚Üí FastAPI ‚Üí Sends task to Redis ‚Üí Celery worker ‚Üí Creates PDF ‚Üí Uploads to S3 ‚Üí Returns presigned URL

---

## üéØ Our Implementation: Kestra Orchestration

We use **Kestra** instead of Celery for workflow orchestration.

### Why Kestra?

| Celery | Kestra |
|--------|--------|
| Python functions | YAML workflows |
| Needs Redis | No Redis needed |
| No built-in UI | Visual workflow UI |
| Manual AI integration | Built-in OpenAI plugin |

### Our Data Flow (with Kestra)

```
1. USER UPLOADS VIDEO
   ‚Üì
   FastAPI ‚Üí Cloudinary (storage)
   
2. USER STARTS ANALYSIS (triggers Kestra)
   ‚Üì
   Kestra Workflow starts
   
3. KESTRA: Start Processing
   ‚Üì
   Calls POST /api/v1/processing/start
   FastAPI runs YOLO + ByteTrack
   
4. KESTRA: Wait & Poll
   ‚Üì
   Polls GET /api/v1/processing/status/{id}
   Until status = "completed"
   
5. KESTRA: Get Collision Data
   ‚Üì
   Calls GET /api/v1/kestra/project/{id}/collision-data
   Gets collision details (IoU, severity, frames)
   
6. KESTRA: AI Analysis (OpenAI GPT-4)
   ‚Üì
   Built-in OpenAI plugin summarizes collision
   Generates natural language report
   
7. KESTRA: Decision Making
   ‚Üì
   If severity == "severe" ‚Üí Priority alert
   If severity == "moderate" ‚Üí Standard processing
   If severity == "minor" ‚Üí Log for records
   
8. KESTRA: Save Summary
   ‚Üì
   Calls POST /api/v1/kestra/project/{id}/save-summary
   Persists AI report to database
```

### Kestra Workflow Visual

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KESTRA WORKFLOW UI                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ Check   ‚îÇ ‚Üí ‚îÇ Process ‚îÇ ‚Üí ‚îÇ Get     ‚îÇ ‚Üí ‚îÇ AI      ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ Status  ‚îÇ   ‚îÇ Video   ‚îÇ   ‚îÇ Collisn ‚îÇ   ‚îÇ Summary ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ ‚úÖ      ‚îÇ   ‚îÇ ‚úÖ      ‚îÇ   ‚îÇ ‚úÖ      ‚îÇ   ‚îÇ ‚úÖ      ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                              ‚îÇ              ‚îÇ
‚îÇ                                              ‚ñº              ‚îÇ
‚îÇ                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ                                       ‚îÇ Decide  ‚îÇ          ‚îÇ
‚îÇ                                       ‚îÇ Action  ‚îÇ          ‚îÇ
‚îÇ                                       ‚îÇ ‚úÖ      ‚îÇ          ‚îÇ
‚îÇ                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                            ‚îÇ               ‚îÇ
‚îÇ                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                        ‚îÇ                   ‚îÇ            ‚îÇ  ‚îÇ
‚îÇ                        ‚ñº                   ‚ñº            ‚ñº  ‚îÇ
‚îÇ                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   ‚îÇ SEVERE ‚îÇ         ‚îÇMODERATE‚îÇ   ‚îÇ MINOR  ‚îÇ
‚îÇ                   ‚îÇ Alert! ‚îÇ         ‚îÇ Normal ‚îÇ   ‚îÇ Log    ‚îÇ
‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Hackathon Track: Kestra

This implementation satisfies:

‚úÖ **"Best project using Kestra's built-in AI Agent to summarise data from other systems"**
- Uses Kestra's OpenAI plugin to summarize collision data
- Fetches data from our FastAPI (the "other system")

‚úÖ **"Bonus: Agent makes decisions based on summarised data"**
- Severity-based decision making (severe ‚Üí alert, moderate ‚Üí normal, minor ‚Üí log)

### Kestra Quick Start

```bash
# 1. Start Kestra
docker run --rm -it -p 8080:8080 kestra/kestra:latest server local

# 2. Open UI
open http://localhost:8080

# 3. Import workflow
# Copy kestra/workflows/accident-analysis.yaml

# 4. Add OpenAI secret
# Settings > Secrets > Add OPENAI_API_KEY

# 5. Run workflow with project_id
```

---

## üöÄ Oumi VLM (Hackathon Track - Demo Only)

We also implemented **Oumi VLM** for the Oumi hackathon track.

**Status:** ‚ö†Ô∏è Code preserved but NOT active (requires 18GB+ RAM)

The Oumi code is kept in:
- `src/services/oumi_vlm.py` - VLM inference
- `src/services/oumi_rl_finetuning.py` - RL fine-tuning
- `src/api/routes/vlm_analysis_route.py` - API endpoints (commented out)

**What it would do:**
1. Analyze collision frames with Vision-Language Model
2. Generate natural language descriptions
3. Fine-tune model with RLHF

**Why not active:**
- Qwen2-VL-2B requires ~18GB RAM
- Our hardware has 16GB
- Kept code for demo purposes

---

## üéì Learning Resources

- **YOLO:** https://github.com/ultralytics/ultralytics
- **ByteTrack:** https://github.com/ifzhang/ByteTrack
- **Homography:** https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html
- **Kestra:** https://kestra.io/docs
- **OpenAI:** https://platform.openai.com/docs
- **FastAPI:** https://fastapi.tiangolo.com/

---

*This document explains the complete workflow of the accident reconstruction tool in simple terms.*

